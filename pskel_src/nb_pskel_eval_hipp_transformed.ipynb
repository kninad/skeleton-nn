{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../')\n",
    "import utils.misc as workspace\n",
    "from SkelPointNet import SkelPointNet \n",
    "from DataUtil import PCDataset, EllipsoidPcDataset, TestBinaryImageData, HippocampiProcessedData\n",
    "import FileRW as rw\n",
    "import DistFunc as DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(log_path, batch_id, input_xyz, skel_xyz, skel_r, label_xyz):\n",
    "    batch_size = skel_xyz.size()[0]\n",
    "    batch_id = batch_id.numpy()\n",
    "    input_xyz_save = input_xyz.detach().cpu().numpy()\n",
    "    skel_xyz_save = skel_xyz.detach().cpu().numpy()\n",
    "    skel_r_save = skel_r.detach().cpu().numpy()\n",
    "    label_xyz_save = label_xyz.detach().cpu().numpy()\n",
    "    for i in range(batch_size):\n",
    "        save_name_input = os.path.join(log_path, f\"val_{batch_id[i]}_input.ply\")\n",
    "        save_name_sphere = os.path.join(log_path, f\"val_{batch_id[i]}_sphere.obj\")\n",
    "        save_name_center = os.path.join(log_path, f\"val_{batch_id[i]}_center.ply\")\n",
    "        save_name_label = os.path.join(log_path, f\"val_{batch_id[i]}_label.ply\")\n",
    "        rw.save_ply_points(input_xyz_save[i], save_name_input)\n",
    "        rw.save_spheres(skel_xyz_save[i], skel_r_save[i], save_name_sphere)\n",
    "        rw.save_ply_points(skel_xyz_save[i], save_name_center)\n",
    "        rw.save_ply_points(label_xyz_save[i], save_name_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_skeletal(batch_id, label_xyz, skel_xyz):\n",
    "    \"\"\"\n",
    "    input_xyz : the g.t srep points\n",
    "    skel_xyz : predicted skeletal points\n",
    "    \"\"\"\n",
    "    batch_size = skel_xyz.size()[0]\n",
    "    batch_id = batch_id.numpy()\n",
    "    label_xyz_save = label_xyz.detach().cpu().numpy()\n",
    "    skel_xyz_save = skel_xyz.detach().cpu().numpy()\n",
    "    cd = 0\n",
    "    hd = 0\n",
    "    for i in range(batch_size):\n",
    "        cd += DF.compute_pc_chamfer(label_xyz_save[i], skel_xyz_save[i])\n",
    "        hd += DF.compute_pc_haussdorff(label_xyz_save[i], skel_xyz_save[i])\n",
    "    # cd /= batch_size\n",
    "    # hd /= batch_size\n",
    "    return cd, hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"gt-full5000-pskel100-finetune_hipp\"\n",
    "experiment_dir = os.path.join(\"../experiments/\", EXP_NAME)\n",
    "# split_file = 'val_split.txt'\n",
    "checkpoint = 'latest'\n",
    "\n",
    "with open(os.path.join(experiment_dir, \"specs.json\"), \"r\") as f:\n",
    "    specs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_num = specs[\"InputPointNum\"]\n",
    "skelpoint_num = specs[\"SkelPointNum\"]\n",
    "to_normalize = specs[\"Normalize\"]\n",
    "gpu = \"0\"\n",
    "model_skel = SkelPointNet(\n",
    "    num_skel_points=skelpoint_num, input_channels=0, use_xyz=True\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "    print(\"GPU Number:\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_skel.cuda()\n",
    "    model_skel.eval()\n",
    "\n",
    "# Load the saved model\n",
    "model_epoch = workspace.load_model_checkpoint(\n",
    "    experiment_dir, checkpoint, model_skel\n",
    ")\n",
    "print(f\"Evaluating model on using checkpoint={checkpoint} and epoch={model_epoch}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and evaluate\n",
    "# Assume Training/Test split file (given as cmd line arg) will be present in the experiment dir\n",
    "data_dir = \"../data/hippocampi_processed/\"\n",
    "\n",
    "# load data and train\n",
    "data_list = sorted(\n",
    "    glob.glob(os.path.join(data_dir, \"surfaces\", \"*_surf_SPHARM.vtk\"))\n",
    ")\n",
    "\n",
    "label_list = sorted(glob.glob(os.path.join(data_dir, \"sreps\", \"*.srep.json\")))\n",
    "\n",
    "idx_end = int(len(data_list) * 0.9)\n",
    "data_list_eval = data_list[idx_end:]\n",
    "label_list_eval = label_list[idx_end:]\n",
    "\n",
    "eval_data = HippocampiProcessedData(\n",
    "    data_list_eval, label_list_eval, point_num, load_in_ram=True\n",
    ")\n",
    "\n",
    "# eval_data = PCDataset(pc_list, data_dir, point_num, to_normalisze)\n",
    "data_loader = DataLoader(\n",
    "    dataset=eval_data, batch_size=1, shuffle=False, drop_last=False\n",
    ")\n",
    "\n",
    "eval_save_dir = os.path.join(experiment_dir, workspace.evaluation_subdir, \"hipp\")\n",
    "rw.check_and_create_dirs([eval_save_dir])\n",
    "\n",
    "overall_loss = 0\n",
    "label_loss_cd = 0\n",
    "label_loss_hd = 0\n",
    "for _, batch_data in enumerate(tqdm(data_loader)):\n",
    "    batch_id, batch_pc, batch_label = batch_data\n",
    "    batch_id = batch_id\n",
    "    batch_pc = batch_pc.cuda().float()\n",
    "    with torch.no_grad():\n",
    "        skel_xyz, skel_r, _ = model_skel(batch_pc, compute_graph=False)\n",
    "        loss = model_skel.get_sampling_loss(batch_pc, skel_xyz, skel_r)        \n",
    "        overall_loss += loss.item()\n",
    "        cd_batch, hd_batch = compute_metrics_skeletal(\n",
    "            batch_id,\n",
    "            batch_label,\n",
    "            skel_xyz\n",
    "        )\n",
    "        label_loss_cd += cd_batch\n",
    "        label_loss_hd += hd_batch\n",
    "    # save_results(eval_save_dir, batch_id, batch_pc, skel_xyz, skel_r, batch_label)\n",
    "overall_loss /= len(data_loader)\n",
    "label_loss_cd /= len(data_loader)\n",
    "label_loss_hd /= len(data_loader)\n",
    "\n",
    "print(len(data_loader))\n",
    "print(len(eval_data))\n",
    "print(overall_loss)\n",
    "print(label_loss_cd)\n",
    "print(label_loss_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pskel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f698cd4174c965919b3de2257892c70c2ba2f080fdf4a93124a46aa180827522"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
