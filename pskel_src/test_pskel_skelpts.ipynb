{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../')\n",
    "import utils.misc as workspace\n",
    "from SkelPointNet import SkelPointNet \n",
    "from DataUtil import PCDataset, EllipsoidPcDataset\n",
    "import FileRW as rw\n",
    "import DistFunc as DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"full-5000-pskel-500\"\n",
    "experiment_dir = os.path.join(\"../experiments/\", EXP_NAME)\n",
    "split_file = 'val_split.txt'\n",
    "checkpoint = 'latest'\n",
    "\n",
    "with open(os.path.join(experiment_dir, \"specs.json\"), \"r\") as f:\n",
    "    specs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = specs[\"DataSource\"]\n",
    "point_num = specs[\"InputPointNum\"]\n",
    "skelpoint_num = specs[\"SkelPointNum\"]\n",
    "to_normalize = specs[\"Normalize\"]\n",
    "# Assume Training/Test split file (given as cmd line arg) will be present in the experiment dir\n",
    "pc_list_file = os.path.join(experiment_dir, split_file)\n",
    "gpu = \"0\"\n",
    "model_skel = SkelPointNet(\n",
    "    num_skel_points=skelpoint_num, input_channels=0, use_xyz=True\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
    "    print(\"GPU Number:\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_skel.cuda()\n",
    "    model_skel.eval()\n",
    "\n",
    "# Load the saved model\n",
    "model_epoch = workspace.load_model_checkpoint(\n",
    "    experiment_dir, checkpoint, model_skel\n",
    ")\n",
    "print(f\"Evaluating model on using checkpoint={checkpoint} and epoch={model_epoch}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and evaluate\n",
    "pc_list = rw.load_data_id(pc_list_file)\n",
    "\n",
    "eval_data = PCDataset(pc_list, data_dir, point_num, to_normalize)\n",
    "data_loader = DataLoader(\n",
    "    dataset=eval_data, batch_size=1, shuffle=False, drop_last=False\n",
    ")\n",
    "\n",
    "overall_loss = 0\n",
    "for _, batch_data in enumerate(tqdm(data_loader)):\n",
    "    batch_id, batch_pc = batch_data\n",
    "    batch_id = batch_id\n",
    "    batch_pc = batch_pc.cuda().float()\n",
    "    with torch.no_grad():\n",
    "        skel_xyz, skel_r, _ = model_skel(batch_pc, compute_graph=False)\n",
    "        loss = model_skel.get_sampling_loss(batch_pc, skel_xyz, skel_r)        \n",
    "        overall_loss += loss.item()\n",
    "overall_loss /= len(data_loader)\n",
    "\n",
    "print(overall_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_skeletal(batch_id, input_xyz, skel_xyz):\n",
    "    \"\"\"\n",
    "    input_xyz : the g.t srep points\n",
    "    skel_xyz : predicted skeletal points\n",
    "    \"\"\"\n",
    "    batch_size = skel_xyz.size()[0]\n",
    "    batch_id = batch_id.numpy()\n",
    "    input_xyz_save = input_xyz.detach().cpu().numpy()\n",
    "    skel_xyz_save = skel_xyz.detach().cpu().numpy()\n",
    "    cd = 0\n",
    "    hd = 0\n",
    "    for i in range(batch_size):\n",
    "        cd += DF.compute_pc_chamfer(input_xyz_save[i], skel_xyz_save[i])\n",
    "        hd += DF.compute_pc_haussdorff(input_xyz_save[i], skel_xyz_save[i])\n",
    "    cd /= batch_size\n",
    "    hd /= batch_size\n",
    "    return cd, hd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = rw.load_label_id(pc_list_file)\n",
    "label_dir = \"../data/ellipsoid_data_5000/pointcloud_srep/\"\n",
    "\n",
    "label_data = EllipsoidPcDataset(\n",
    "    pc_list, label_list, \n",
    "    data_dir, label_dir,\n",
    "    point_num, to_normalize\n",
    "    )\n",
    "\n",
    "label_loader = DataLoader(label_data, batch_size=1, shuffle=False, drop_last=False)\n",
    "\n",
    "overall_cd = 0\n",
    "overall_hd = 0\n",
    "for _, batch_data,  in enumerate(tqdm(label_loader)):\n",
    "    batch_id, batch_pc, batch_label = batch_data\n",
    "    batch_id = batch_id\n",
    "    batch_pc = batch_pc.cuda().float()\n",
    "    with torch.no_grad():\n",
    "        skel_xyz, skel_r, _ = model_skel(batch_pc, compute_graph=False)\n",
    "        avg_cd_batch, avg_hd_batch = compute_metrics_skeletal(\n",
    "                                        batch_id, \n",
    "                                        batch_label, \n",
    "                                        skel_xyz)\n",
    "        overall_cd += avg_cd_batch\n",
    "        overall_hd += avg_hd_batch\n",
    "\n",
    "overall_cd /= len(data_loader)\n",
    "overall_hd /= len(data_loader)\n",
    "print(overall_cd, overall_hd)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pskel')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f698cd4174c965919b3de2257892c70c2ba2f080fdf4a93124a46aa180827522"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
